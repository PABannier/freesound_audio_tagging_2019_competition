{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import imgaug.augmenters as iaa\n",
    "\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_augmenter():\n",
    "    seq = iaa.Sequential([\n",
    "        iaa.Crop(px=(0, 16)),\n",
    "        iaa.Fliplr(0.5),\n",
    "        iaa.GaussianBlur(sigma=(0, 3.0))\n",
    "    ])\n",
    "    \n",
    "    def augment(images):\n",
    "        return seq.augment(images.transpose(0, 2, 3, 1)).transpose(0, 2, 3, 1)\n",
    "    \n",
    "    return augment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sharpen(x, T):\n",
    "    temp = x**(1/T)\n",
    "    return temp / temp.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup(x1, x2, y1, y2, alpha):\n",
    "    beta = np.random.beta(alpha, -alpha)\n",
    "    x = beta * x1 + (1 - beta) * x2\n",
    "    y = beta * y1 + (1 - beta) * y2\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixmatch(x, y, u, model, augment_fn, T=0.5, K=2, alpha=0.75):\n",
    "    xb = augment_fn(x)\n",
    "    ub = [augment_fn(u) for _ in range(K)]\n",
    "    qb = sharpen(sum(map(lambda i: model(i), ub)) / K, T)\n",
    "    Ux = np.concatenate(ub, axis=0)\n",
    "    Uy = np.concatenate([qb for _ in range(K)], axis=0)\n",
    "    indices = np.random.shuffle(np.arange(len(xb) + len(Ux)))\n",
    "    Wx = np.concatenate([Ux, xb], axis=0)[indices]\n",
    "    Wy = np.concatenate([qb, y], axis=0)[indices]\n",
    "    X, p = mixup(xb, Wx[:len(xb)], y, Wy[:len(xb)], alpha)\n",
    "    U, q = mixup(Ux, Wx[len(xb):], Uy, Wy[len(xb):], alpha)\n",
    "    return X, U, p, q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixMatchLoss(torch.nn.Module):\n",
    "    def __init__(self, lambda_u=100):\n",
    "        self.lambda_u = lambda_u\n",
    "        self.xent = torch.nn.CrossEntropyLoss()\n",
    "        self.mse = torch.nn.MSELoss()\n",
    "        super(MixMatchLoss, self).__init__()\n",
    "    \n",
    "    def forward(self, X, U, p, q, model):\n",
    "        X_ = np.concatenate([X, U], axis=1)\n",
    "        preds = model(X_)\n",
    "        return self.xent(preds[:len(p)], p) + self.lambda_u * self.mse(preds[len(p):], q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.zeros_(m.bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.avg_pool2d(x, 2)\n",
    "        return x\n",
    "    \n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, num_classes=1000): # <======== modificaition to comply fast.ai\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            ConvBlock(in_channels=3, out_channels=64),\n",
    "            ConvBlock(in_channels=64, out_channels=128),\n",
    "            ConvBlock(in_channels=128, out_channels=256),\n",
    "            ConvBlock(in_channels=256, out_channels=512),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1)) # <======== modificaition to comply fast.ai\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.PReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(128, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        #x = torch.mean(x, dim=3)   # <======== modificaition to comply fast.ai\n",
    "        #x, _ = torch.max(x, dim=2) # <======== modificaition to comply fast.ai\n",
    "        x = self.avgpool(x)         # <======== modificaition to comply fast.ai\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def borrowed_model(pretrained=False, **kwargs):\n",
    "    return Classifier(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_generator(x, y=None, batch_size=32, shuffle=True):\n",
    "    i = 0\n",
    "    all_indices = np.random.shuffle(np.arange(len(x))) if shuffle else \\\n",
    "                                                               np.arange(len(x))\n",
    "    while(True):\n",
    "        indices = all_indices[i:i+batch_size]\n",
    "        if y is not None:\n",
    "            yield x[indices], y[indices]\n",
    "        yield x[indices]\n",
    "        i = (i + batch_size) % len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixmatch_wrapper(x, y, u, model, batch_size=32):\n",
    "    augment_fn = get_augmenter()\n",
    "    train_generator = basic_generator(x, y, batch_size)\n",
    "    unlabeled_generator = basic_generator(u, batch_size=batch_size)\n",
    "    while(True):\n",
    "        xi, yi = next(train_generator)\n",
    "        ui = next(unlabeled_generator)\n",
    "        yield mixmatch(xi, yi, ui, model, augment_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_torch(*args, device='cuda'):\n",
    "    convert_fn = lambda x: torch.from_numpy(x).to(device)\n",
    "    return list(map(convert_fn, args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_gen, test_iters):\n",
    "    acc = []\n",
    "    for i, (x, y) in enumerate(test_gen):\n",
    "        x = to_torch(x)\n",
    "        pred = model(x).to('cpu').argmax(axis=1)\n",
    "        acc.append(np.mean(pred == y.argmax(axis=1)))\n",
    "        if i == test_iters:\n",
    "            break\n",
    "    print('Accuracy was : {}'.format(np.mean(acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(loss_history):\n",
    "    print('Average loss in last epoch was : {}'.format(np.mean(loss_history)))\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(model, iter, train_iters):\n",
    "    torch.save(model.state_dict(), 'model_{}.pth'.format(train_iters // iters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(model, train_gen, test_gen, epochs, train_iters, test_iters, device):\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = MixMatchLoss()\n",
    "    loss_history = []\n",
    "    for i, (x, u, p, q) in enumerate(train_gen):\n",
    "        if i % train_iters == 0:\n",
    "            loss_history = report(loss_history)\n",
    "            test(model, test_gen, test_iters)\n",
    "            save(model, i, train_iters)\n",
    "            if i // train_iters == epochs:\n",
    "                return\n",
    "        else:\n",
    "            optim.zero_grad()\n",
    "            x, u, p, q = to_torch(x, u, p, q, device=device)\n",
    "            loss = loss_fn(x, u, p, q, model)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            loss_history.append(loss.to('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(borrowed_model, )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
